{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fintech Knowledge Copilot - Nigeria (RAG System)\n",
        "\n",
        "This notebook demonstrates **Retrieval-Augmented Generation (RAG)** using internal policy documents from a Nigerian fintech company.\n",
        "\n",
        "**Learning Objectives:**\n",
        "- Understand how RAG works (retrieve relevant documents, then generate answers)\n",
        "- See how embeddings enable semantic search\n",
        "- Practice building a simple document Q&A system\n",
        "- Learn about the importance of citations and source transparency\n",
        "\n",
        "**One-Sentence Summary (Memorable)**\n",
        "- *LLM*: A powerful language generator trained on massive text.\n",
        "- *RAG*: A method that forces the LLM to answer using your verified data."
      ],
      "metadata": {
        "id": "jDUaCZNi0ltD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating Ã  response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. RAG extends the already powerful capabilities of LLMs to specific domains or an organization's internal knowledge base, all without the need to retrain the model. It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts."
      ],
      "metadata": {
        "id": "grQj2oRc31L3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Install Required Libraries"
      ],
      "metadata": {
        "id": "U7hT9qx429S5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries (runs once per runtime)\n",
        "!pip install -q transformers sentence-transformers torch"
      ],
      "metadata": {
        "id": "tHuFsd8jnuoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Mount Google Drive and Load Documents\n",
        "\n",
        "**Before running this notebook:**\n",
        "1. Download the `fintech_docs` folder (contains kyc_policy.txt, dispute_resolution.txt, product_faqs.txt)\n",
        "2. Upload it to your Google Drive (e.g. `My Drive/fintech_docs`)\n",
        "3. Run the cells below to mount Drive and load the documents"
      ],
      "metadata": {
        "id": "mqrJK2ci_v7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive so we can load the policy documents from your Drive.\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "A0nyH571Ih8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDmD76TfmZjO"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Path to the fintech_docs folder in your Google Drive.\n",
        "# Change this if you uploaded the folder to a different location.\n",
        "# Example: \"/content/drive/MyDrive/fintech_docs\"\n",
        "DOCS_DRIVE_PATH = \"/content/drive/MyDrive/RBSDS/LLM/fintech_docs\"\n",
        "\n",
        "import os\n",
        "if os.path.exists(DOCS_DRIVE_PATH):\n",
        "    print(f\"Found docs folder at: {DOCS_DRIVE_PATH}\")\n",
        "    print(\"Files:\", os.listdir(DOCS_DRIVE_PATH))\n",
        "else:\n",
        "    print(\"Folder not found! Please:\")\n",
        "    print(\"1. Download the fintech_docs folder (kyc_policy.txt, dispute_resolution.txt, product_faqs.txt)\")\n",
        "    print(\"2. Upload it to your Google Drive (e.g. My Drive/fintech_docs)\")\n",
        "    print(\"3. Update DOCS_DRIVE_PATH above if you used a different folder name or path\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Documents and Split into Paragragraphs"
      ],
      "metadata": {
        "id": "H4bee1qnN0P5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell reads all .txt files from the docs folder (in your Drive) and splits each one\n",
        "# into short paragraphs that we will use for retrieval.\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Use the Drive path from Step 2. Change if your folder is elsewhere.\n",
        "DOCS_DIR = Path(DOCS_DRIVE_PATH)\n",
        "\n",
        "def load_documents():\n",
        "    docs = []\n",
        "    for fname in os.listdir(DOCS_DIR):\n",
        "        full_path = DOCS_DIR / fname\n",
        "        if not full_path.is_file():\n",
        "            continue\n",
        "\n",
        "        with open(full_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read()\n",
        "\n",
        "        # Split on blank lines to get paragraphs.\n",
        "        paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
        "\n",
        "        for idx, para in enumerate(paragraphs):\n",
        "            docs.append(\n",
        "                {\n",
        "                    \"filename\": fname,\n",
        "                    \"paragraph_index\": idx,\n",
        "                    \"text\": para,\n",
        "                }\n",
        "            )\n",
        "    return docs\n",
        "\n",
        "if not DOCS_DIR.exists():\n",
        "    print(\"ERROR: Docs folder not found at\", DOCS_DRIVE_PATH)\n",
        "    print(\"Please upload the fintech_docs folder to your Google Drive and update DOCS_DRIVE_PATH in Step 2.\")\n",
        "else:\n",
        "    docs = load_documents()\n",
        "    print(f\"Loaded {len(docs)} paragraphs from {DOCS_DIR}\")\n",
        "    print(docs[:3])  # Show first few paragraphs as a sample"
      ],
      "metadata": {
        "id": "b9uaDEPLODcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 4: Load Embedding Model and FLAN-T5"
      ],
      "metadata": {
        "id": "5Gjp0hjcSMnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell loads:\n",
        "# - a sentence embedding model for retrieval (MiniLM)\n",
        "# - the FLAN-T5-small model for generation\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "embed_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "gen_model_name = \"google/flan-t5-small\"\n",
        "\n",
        "embedder = SentenceTransformer(embed_model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(gen_model_name)\n",
        "gen_model = AutoModelForSeq2SeqLM.from_pretrained(gen_model_name)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "gen_model.to(device)\n",
        "\n",
        "print(\"Embedding model:\", embed_model_name)\n",
        "print(\"Generation model:\", gen_model_name)\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "id": "7GW6Ve-WT92J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Build Embeddings and Define RAG Functions"
      ],
      "metadata": {
        "id": "dsDTUHCNaFQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell builds embeddings for all paragraphs and defines helper functions\n",
        "# for retrieval and answer generation.\n",
        "\n",
        "import torch\n",
        "\n",
        "# Encode all paragraph texts into vectors once.\n",
        "paragraph_texts = [d[\"text\"] for d in docs]\n",
        "paragraph_embeddings = embedder.encode(paragraph_texts, convert_to_tensor=True)\n",
        "\n",
        "print(\"Built paragraph embeddings with shape:\", paragraph_embeddings.shape)\n",
        "\n",
        "def retrieve_context(query: str, top_k: int = 3):\n",
        "    \"\"\"\n",
        "    Given a question string, find the top_k most similar paragraphs.\n",
        "    \"\"\"\n",
        "    # Embed the question into the same vector space.\n",
        "    query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
        "\n",
        "    # Compute cosine similarity between question and each paragraph.\n",
        "    scores = util.cos_sim(query_embedding, paragraph_embeddings)[0]\n",
        "\n",
        "    # Get indices of the best-matching paragraphs.\n",
        "    top_indices = torch.topk(scores, k=min(top_k, len(docs))).indices.tolist()\n",
        "\n",
        "    # Collect paragraphs with scores.\n",
        "    retrieved = []\n",
        "    for idx in top_indices:\n",
        "        item = docs[idx].copy()\n",
        "        item[\"score\"] = float(scores[idx])\n",
        "        retrieved.append(item)\n",
        "    return retrieved\n",
        "\n",
        "\n",
        "def generate_answer_from_context(question: str, retrieved_paragraphs):\n",
        "    \"\"\"\n",
        "    Use FLAN-T5 to answer the question based only on the retrieved paragraphs.\n",
        "    \"\"\"\n",
        "    # Join the retrieved paragraphs into one context string, tagging with filenames.\n",
        "    context_lines = []\n",
        "    for para in retrieved_paragraphs:\n",
        "        context_lines.append(f\"[{para['filename']}] {para['text']}\")\n",
        "    context_text = \"\\n\\n\".join(context_lines)\n",
        "\n",
        "    # Build the instruction prompt for the model.\n",
        "    prompt = (\n",
        "        \"You are a compliance and operations assistant at a Nigerian fintech company. \"\n",
        "        \"You receive internal policy documents as context. \"\n",
        "        \"Answer the employee's question ONLY using the information in the context. \"\n",
        "        \"If the answer is not clearly in the context, say you do not know and \"\n",
        "        \"recommend contacting the compliance team.\\n\\n\"\n",
        "        f\"Context:\\n{context_text}\\n\\n\"\n",
        "        f\"Question: {question}\\n\\n\"\n",
        "        \"Answer in simple English for a non-technical employee:\"\n",
        "    )\n",
        "\n",
        "    # Tokenize the prompt and move it to the correct device.\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=768,\n",
        "        truncation=True,\n",
        "    ).to(device)\n",
        "\n",
        "    # T5/FLAN-T5 needs decoder_start_token_id and pad_token_id for reliable generation.\n",
        "    # Without these, the model can sometimes produce empty output.\n",
        "    if tokenizer.pad_token_id is None:\n",
        "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    # Ask the model to generate an answer.\n",
        "    outputs = gen_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=200,\n",
        "        min_length=5,\n",
        "        decoder_start_token_id=tokenizer.pad_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "    # Decode the generated tokens back to text.\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "\n",
        "    # Fallback if model returns empty (can happen with some transformer versions).\n",
        "    if not answer or len(answer) < 3:\n",
        "        answer = (\n",
        "            \"Based on the retrieved documents: \"\n",
        "            + retrieved_paragraphs[0][\"text\"][:300]\n",
        "            + (\"...\" if len(retrieved_paragraphs[0][\"text\"]) > 300 else \"\")\n",
        "        )\n",
        "    return answer"
      ],
      "metadata": {
        "id": "7y45_2jxT9zD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Interactive Q&A Interface"
      ],
      "metadata": {
        "id": "TNuYGDGHgkcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell creates a small question-answering interface using Colab widgets.\n",
        "# The flow is:\n",
        "# 1. User types a question.\n",
        "# 2. We retrieve the top 3 relevant paragraphs.\n",
        "# 3. We call FLAN-T5 to answer the question from those paragraphs.\n",
        "# 4. We display the answer and the sources.\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Text box where the user types their question.\n",
        "question_box = widgets.Textarea(\n",
        "    value=\"What ID documents are required for Tier 2 wallets?\",\n",
        "    placeholder=\"Try: What are Tier 1 limits? How long for dispute refunds? How to fund wallet? What for chargebacks?\",\n",
        "    description=\"Question:\",\n",
        "    layout=widgets.Layout(width=\"100%\", height=\"80px\"),\n",
        "    style={\"description_width\": \"initial\"},\n",
        ")\n",
        "\n",
        "# Button to run the retrieval and generation.\n",
        "ask_button = widgets.Button(\n",
        "    description=\"Ask the Fintech Copilot\",\n",
        "    button_style=\"primary\",\n",
        ")\n",
        "\n",
        "# Output area where we will print the answer and sources.\n",
        "output = widgets.Output()\n",
        "\n",
        "\n",
        "def on_ask_button_clicked(b):\n",
        "    with output:\n",
        "        # Clear previous content (wait=True helps avoid flicker/empty display).\n",
        "        output.clear_output(wait=True)\n",
        "\n",
        "        question = question_box.value.strip()\n",
        "        if not question:\n",
        "            print(\"Please type a question first.\")\n",
        "            return\n",
        "\n",
        "        print(\"Employee question:\\n\", question, \"\\n\")\n",
        "\n",
        "        # Step 1: retrieve top paragraphs.\n",
        "        retrieved = retrieve_context(question, top_k=3)\n",
        "\n",
        "        # Step 2: generate answer using FLAN-T5.\n",
        "        print(\"Generating answer from retrieved context...\\n\")\n",
        "        answer = generate_answer_from_context(question, retrieved)\n",
        "\n",
        "        # Show the answer.\n",
        "        display(Markdown(\"### Answer\"))\n",
        "        print(answer, \"\\n\")\n",
        "\n",
        "        # Show the sources so the user knows where information came from.\n",
        "        display(Markdown(\"### Sources from internal documents\"))\n",
        "        for para in retrieved:\n",
        "            print(\n",
        "                f\"- {para['filename']} (score={para['score']:.2f}), \"\n",
        "                f\"paragraph {para['paragraph_index']}\"\n",
        "            )\n",
        "            print(para[\"text\"])\n",
        "            print()\n",
        "\n",
        "        print(\n",
        "            \"Note: This is an educational demo only and not legal advice. \"\n",
        "            \"Always confirm important decisions with the compliance team.\"\n",
        "        )\n",
        "\n",
        "\n",
        "# Connect the button to the click handler.\n",
        "ask_button.on_click(on_ask_button_clicked)\n",
        "\n",
        "# Display the UI elements.\n",
        "display(question_box, ask_button, output)"
      ],
      "metadata": {
        "id": "Uncw-VGPgmSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FAQs\n",
        "1.\tWhat ID documents are required for Tier 2 wallets?\n",
        "2.\tWhat are the transaction limits for Tier 1 accounts?\n",
        "3.\tHow long do we have to acknowledge a customer dispute?\n",
        "4.\tHow can customers fund their wallet?\n",
        "5.\tWhat is the refund timeline for failed NIP transfers?\n",
        "6.\tWhat documents are needed for Tier 1 KYC?\n",
        "7.\tHow many days does a customer have to submit a chargeback?\n",
        "8.\tWhat can customers do with their wallet card?\n",
        "9.\tWhat are the Tier 3 enhanced due diligence requirements?\n",
        "10.\tHow should we communicate with customers during a dispute?"
      ],
      "metadata": {
        "id": "oc9Gv-CAhw8D"
      }
    }
  ]
}